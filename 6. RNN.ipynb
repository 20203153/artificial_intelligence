{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 다음 참고자료를 바탕으로 작성되었습니다.\n",
        "### https://github.com/lih0905/korean-pytorch-sentiment-analysis"
      ],
      "metadata": {
        "id": "WE72XWDOwp2v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XCu7cRzRYAP"
      },
      "outputs": [],
      "source": [
        "# 한국어 자연어처리 패키지 KoNLPy 및 형태소 분석기 MeCab 설치\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/SOMJANG/Mecab-ko-for-Google-Colab/refs/heads/master/install_mecab-ko_on_colab_light_220429.sh\", filename=\"mecab.sh\")\n",
        "!bash mecab.sh\n",
        "\n",
        "# Pytorch 및 torchtext 라이브러리 다운그레이드 (최신버전에서는 충돌 이슈 발생)\n",
        "!pip install torch==1.12.0 torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 위의 셀을 실행한 후, <런타임 -> 세션 다시 시작> 클릭하여 런타임 재실행\n"
      ],
      "metadata": {
        "id": "X9nbPWZNRywF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요 라이브러리들 import\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import random\n",
        "\n",
        "torch.manual_seed(2024)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "Fj1S62cJUkq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nn.RNN 및 nn.LSTM 이해, 파라미터 개수 확인"
      ],
      "metadata": {
        "id": "Kb4eGJ2kwVTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = nn.RNN(input_size=20, hidden_size=25)\n",
        "print(sum(p.numel() for p in a.parameters()))"
      ],
      "metadata": {
        "id": "fAUdK9GVgur1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oI-HDQuwko5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = nn.RNN(input_size=20, hidden_size=25, bidirectional=True)\n",
        "print(sum(p.numel() for p in b.parameters()))"
      ],
      "metadata": {
        "id": "8EL4ikYpko_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xzrTj6cXkskn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = nn.LSTM(input_size=20, hidden_size=25)\n",
        "print(sum(p.numel() for p in c.parameters()))"
      ],
      "metadata": {
        "id": "ZC2-0NbjksoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4WPm_zfjktB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 네이버 영화리뷰 데이터셋 다운로드 및 전처리"
      ],
      "metadata": {
        "id": "fyDhJ0EVl9cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ],
      "metadata": {
        "id": "-swMia4bSnYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['id','text','label']\n",
        "train_data = pd.read_csv('ratings_train.txt', sep='\\t', names=columns, skiprows=1).dropna() # null데이터 삭제\n",
        "test_data = pd.read_csv('ratings_test.txt', sep='\\t', names=columns, skiprows=1).dropna()"
      ],
      "metadata": {
        "id": "cI6pJnS8VPDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[5:10]"
      ],
      "metadata": {
        "id": "jhyH4YOaVs6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "id": "uu7DvyLVVwOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torchtext dataloader를 활용하기 위해 csv 포맷으로 저장\n",
        "\n",
        "train_data.to_csv('train_data.csv',index=False)\n",
        "test_data.to_csv('test_data.csv',index=False)"
      ],
      "metadata": {
        "id": "dCDdaJCyV1sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장을 형태소 단위로 쪼갬\n",
        "\n",
        "from konlpy.tag import Mecab\n",
        "mecab = Mecab()\n",
        "\n",
        "\n",
        "TEXT = torchtext.data.Field(tokenize=mecab.morphs)\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)\n",
        "\n",
        "fields = {'text': ('text',TEXT), 'label': ('label',LABEL)}\n",
        "# dictionary 형식은 {csv컬럼명 : (데이터 컬럼명, Field이름)}\n",
        "\n",
        "train_dataset, test_dataset = torchtext.data.TabularDataset.splits(\n",
        "                            path = '.',\n",
        "                            train = 'train_data.csv',\n",
        "                            test = 'test_data.csv',\n",
        "                            format = 'csv',\n",
        "                            fields = fields,\n",
        ")"
      ],
      "metadata": {
        "id": "qufKvVcYV-AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vars(train_dataset[5])"
      ],
      "metadata": {
        "id": "0TXXajRHWQus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, valid_dataset = train_dataset.split(random_state=random.seed(2024))\n",
        "print(f'훈련 데이터 수 : {len(train_dataset)}')\n",
        "print(f'검증 데이터 수 : {len(valid_dataset)}')\n",
        "print(f'테스트 데이터 수 : {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "-1JtSjtcWhn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_dataset)\n",
        "print(len(TEXT.vocab))"
      ],
      "metadata": {
        "id": "I9BG-i1BXUV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 10000\n",
        "\n",
        "TEXT.build_vocab(train_dataset, max_size=MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_dataset)"
      ],
      "metadata": {
        "id": "V_LSqX0JXaD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "metadata": {
        "id": "QG0ZNS5mXcz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batchsize = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset, test_dataset),\n",
        "    batch_size = batchsize,\n",
        "    device = 'cuda',\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch = False,\n",
        ")"
      ],
      "metadata": {
        "id": "-f3D_ik6XfyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(next(iter(train_iterator)))\n",
        "\n",
        "print(next(iter(train_iterator)).text)"
      ],
      "metadata": {
        "id": "7jY0HZhIelFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN 모델 정의 및 학습"
      ],
      "metadata": {
        "id": "LozAra2XpZ2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text : [sent_len, batch_size]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded : [sent_len, batch_size, emb_dim]\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        # output : [sent_len, batch_size, hidden_dim]\n",
        "        # hidden : [1, batch_size, hidden_dim]\n",
        "\n",
        "        return self.fc(hidden[-1].squeeze(0)) # [batch_size, output_dim]"
      ],
      "metadata": {
        "id": "XyQdL-g4Xm_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).cuda()\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.BCEWithLogitsLoss()\n"
      ],
      "metadata": {
        "id": "jh7GDD0aXv1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    for batch in train_iterator:\n",
        "        output = model(batch.text).squeeze(1)\n",
        "        loss = criterion(output, batch.label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "num_correct = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        pred = model(batch.text).squeeze(1)\n",
        "        rounded_preds = torch.round(torch.sigmoid(pred))\n",
        "        num_correct += (rounded_preds == batch.label).sum()\n",
        "\n",
        "print(f'Accuracy : {num_correct / len(test_dataset) * 100:.2f}%')"
      ],
      "metadata": {
        "id": "y5HwWicyaCD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 사전 훈련된 단어 임베딩 사용"
      ],
      "metadata": {
        "id": "2k_B0kpIpfRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VOCAB_SIZE = 10000\n",
        "\n",
        "TEXT.build_vocab(train_dataset,\n",
        "                max_size = MAX_VOCAB_SIZE,\n",
        "                vectors = 'fasttext.simple.300d',\n",
        "                unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_dataset)"
      ],
      "metadata": {
        "id": "R9z51exYdjmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(pretrained_embeddings.shape)"
      ],
      "metadata": {
        "id": "MgzH1YC8nJdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN 대신 LSTM 모델 기용하여 학습"
      ],
      "metadata": {
        "id": "pywE2LG5piki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        return self.fc(hidden[-1].squeeze(0))"
      ],
      "metadata": {
        "id": "v7mWPGz7i4tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model = model.cuda()\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.BCEWithLogitsLoss()\n"
      ],
      "metadata": {
        "id": "umCTqkNyiL-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    for batch in train_iterator:\n",
        "        output = model(batch.text).squeeze(1)\n",
        "        loss = criterion(output, batch.label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "num_correct = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        pred = model(batch.text).squeeze(1)\n",
        "        rounded_preds = torch.round(torch.sigmoid(pred))\n",
        "        num_correct += (rounded_preds == batch.label).sum()\n",
        "\n",
        "print(f'Accuracy : {num_correct / len(test_dataset) * 100:.2f}%')"
      ],
      "metadata": {
        "id": "R03pRpFgiTTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 임의의 텍스트에 대해 모델 예측 Score 확인"
      ],
      "metadata": {
        "id": "cHXFLTu2pmEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok for tok in mecab.morphs(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to('cuda')\n",
        "    tensor = tensor.unsqueeze(1) # 배치\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()"
      ],
      "metadata": {
        "id": "I6X23GpIicgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(\"이 영화 진짜 재밌었다!!\")\n"
      ],
      "metadata": {
        "id": "b3_trihoj1Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sentiment(\"이런걸 돈주고 보다니...\")"
      ],
      "metadata": {
        "id": "Z89gvvXNj4IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VdgcfHdkkZeY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}